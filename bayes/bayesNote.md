# 朴素贝叶斯分类法学习笔记
## 背景     
朴素贝叶斯算法基于[贝叶斯定理](https://zh.wikipedia.org/wiki/贝叶斯定理)与特征条件独立假设的分类方法，这里的特征条件独立其实是一个很强的假设。
对于给定的训练数据集，首先基于特征条件独立假设学习输入／输出的联合概率分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。贝
叶斯公式为:    
![pi](http://latex.codecogs.com/png.latex?P(A|B)=\frac{P(B|A)P(A)}{P(B)})   
贝叶斯提供了一个交换条件和结果的方法。在学习贝叶斯定理的时候可以结合[条件概率](https://zh.wikipedia.org/wiki/条件概率)来理解。我们就是在利用贝叶
斯定理来告诉我们输入数据得到某个类别的概率，然后我们根据概率来进行分类。
## 和 logistic 回归的区别    
这里的一个区别之处可以理解为“判别模型”和“生成模型”之间的区别。机器学习所要实现的是基于有限的训练样本集，尽可能准确的估计出后验概率P(Y|X)，大体上来说
主要有两种策略   
* 给定x可以通过直接建模P(Y|X)来预测Y，这样得到的是“判别模型”。logistic回归就是“判别模型”。
* 先对联合概率分布P(Y,X)建模然后由此得到P(Y|X)这样得到的是“生成模型”。朴素贝叶斯分类法就属于“生成模型”。   

朴素贝叶斯法实际上学习到的就是生成数据的机制。
## 朴素贝叶斯算法    
如前所说我们需要对P(X|Y)进行估计，但是我们都知道一条X数据中包括多个特征现实生活中这些特征之间很可能是存在某种关联的，但是我们为来计算方便强行添加了一
个特征条件之间都是独立的假设这样就形成了[朴素贝叶斯分类器](https://zh.wikipedia.org/wiki/朴素贝叶斯分类器)对应公式为:    
![pi](http://latex.codecogs.com/png.latex?P(y|x)=\frac{P(x|y)P(y)}{P(x)}=\frac{P(y)}{P(x)}\prod_{i=1}^{d}P(x_{i}|y))      

这样在的假设使得朴素贝叶斯分类器变的简单，但是也会牺牲一定的分类准确率。具体实现的时候需要注意到当在计算P(x1|y)P(x2|y)P(x3|y)由于受到训练集本身的影响极有可能其中有个数值可能是0这个时候就会让整个运算结果都变成0这个结果显然并不是我们期望的。为了避免这样的情况我们一个是可以给这个相互乘的对象一个非0的默认值，或者是使用“拉普拉斯修正”进行平滑公式如下：     

![pi](http://latex.codecogs.com/png.latex?\hat{P}(y)=\frac{\left&space;|&space;D_{y}&space;\right&space;|&plus;1}{\left&space;|&space;D&space;\right&space;|&plus;N})  
![pi](http://latex.codecogs.com/png.latex?\hat{P}(x_{i}|y)=\frac{\left&space;|&space;D_{y,x_{i}}&space;\right&space;|&plus;1}{\left&space;|&space;D_{y}&space;\right&space;|&plus;N_{i}})   

这里N表示训练集D中可能的类别数，![pi](http://latex.codecogs.com/png.latex?N_{i})表示第i个属性可能的取值数。接着还需要考虑到概率一般比较小，多个相乘以后结果将会出现“下溢”接近于0，无法被使用。我们的处理方式对对朴素贝叶斯分类公式取对数这样就将概率乘的关系转换成了加的关系并且不会影响最终的结果。   
## 参考书籍

《机器学习实战》 Peter Harrington 著 李锐 译    
《统计学习方法》 李航 著   
《机器学习》 周志华 著        
《斯坦福大学公开课：机器学习课程 cs229 吴恩达       
《coursera 机器学习课程》 吴恩达         
