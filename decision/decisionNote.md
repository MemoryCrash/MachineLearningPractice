# 决策树学习笔记     

## 背景
我们在生活中总是会面对很多需要决策或在叫作出判断的事情。一般人进行判断的时候都会根据一些特点(特征)来进行判断。比如电视剧里表现的侦探破案时会更加犯罪现场的各种情况层层筛选去判断信息(这个可以关注‘今日说法’节目)。我们嘛一般就是判断身高是高还是低，年收入是高中还是一般，是否是本地户口，是否有房子。嗯，对的这个是相亲的判断的问题。         
决策树这种监督学习算法也是这样的考虑，从一堆特征中去构造一个这样的决策树使其泛化性能最好(能更好的预测未知分类的数据)就像下面的图形示，内部节点表示一个一个的特征比如身高、户口、收入这些。叶子节点表示分类。我们这里举例是二分类所以我们的分类就是‘是’或者‘否’这样的内容。

<img src = 'https://github.com/MemoryCrash/MachineLearningPractice/blob/master/image/decisionTree.png' width=30% height=30%/>

## 决策树

在生成决策树的时候我们面临这样的问题就是选择什么的特征来进行划分。特征的选取是基于对训练数据具有分类能力的特征，这样可以提高决策树的学习效率。那怎么样的特征就是更具备分类能力的特征呢？我们需要找到一个选择特征的依据。我们依据的是信息增益。

### 信息增益

在信息论与概率统计中熵(entropy)是度量样本集合纯度的一个重要指标。假设集合D中第一k类样本所占的比例为![pi](http://latex.codecogs.com/gif.latex?p_{k})(k=1,2,3...,Y)，则D的信息熵定义为:

![pi](http://latex.codecogs.com/gif.latex?Ent(D)=-\sum_{k=1}^{Y}p_{k}log_{2}p_{k})

Ent(D)的值越小则，D的纯度越高。现在我们找到度量样本集合纯度的方法。现在需要的是找到一种划分效果最好的特征，按照这个思路来进行理解，就是使用某个特征进行划分后的样本集合的信息熵变小了也就是对应到样本集合的纯度在按照某个特征进行划分后得到了提高。当然我们的一个训练样本对应了肯定不止一个特征，所以我们就需要在这些特征中找到能使的样本集合的纯度提高的最多的那个作为分类划分的特征。我们用信息增益表示按照某个特征进行分类后信息纯度提高的量。
## 参考书籍

《机器学习实战》 Peter Harrington 著 李锐 译    
《统计学习方法》 李航 著   
《机器学习》 周志华 著        
《斯坦福大学公开课：机器学习课程 cs229 吴恩达       
《coursera 机器学习课程》 吴恩达 
